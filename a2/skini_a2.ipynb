{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGR-E 516 Assignment 2\n",
    "## Srinivas Kini\n",
    "## skini@iu.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spark libraries\n",
    "import math\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 11:17:08 WARN Utils: Your hostname, Srinivass-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.229 instead (on interface en0)\n",
      "23/04/10 11:17:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 11:17:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session \n",
    "spark = SparkSession.builder.appName('skini-a2').getOrCreate()\n",
    "context = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the parking violations file\n",
    "ny_df = spark.read.options(header=True, inferschema=True).csv(\"ny_parking_violations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summons_number',\n",
       " 'plate_id',\n",
       " 'registration_state',\n",
       " 'plate_type',\n",
       " 'issue_date',\n",
       " 'violation_code',\n",
       " 'vehicle_body_type',\n",
       " 'vehicle_make',\n",
       " 'issuing_agency',\n",
       " 'street_code1',\n",
       " 'street_code2',\n",
       " 'street_code3',\n",
       " 'vehicle_expiration_date',\n",
       " 'violation_location',\n",
       " 'violation_precinct',\n",
       " 'issuer_precinct',\n",
       " 'issuer_code',\n",
       " 'issuer_command',\n",
       " 'issuer_squad',\n",
       " 'violation_time',\n",
       " 'time_first_observed',\n",
       " 'violation_county',\n",
       " 'violation_in_front_of_or_opposite',\n",
       " 'house_number',\n",
       " 'street_name',\n",
       " 'intersecting_street',\n",
       " 'date_first_observed',\n",
       " 'law_section',\n",
       " 'sub_division',\n",
       " 'violation_legal_code',\n",
       " 'days_parking_in_effect____',\n",
       " 'from_hours_in_effect',\n",
       " 'to_hours_in_effect',\n",
       " 'vehicle_color',\n",
       " 'unregistered_vehicle?',\n",
       " 'vehicle_year',\n",
       " 'meter_number',\n",
       " 'feet_from_curb',\n",
       " 'violation_post_code',\n",
       " 'violation_description',\n",
       " 'no_standing_or_stopping_violation',\n",
       " 'hydrant_violation',\n",
       " 'double_parking_violation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming columns for ease of use in Spark SQL\n",
    "col_dict = {}\n",
    "for column in ny_df.columns:\n",
    "    new_col_name = '_'.join(column.lower().split(' '))\n",
    "    ny_df = ny_df.withColumnRenamed(column, new_col_name)\n",
    "ny_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temp. VIEW for Spark SQl\n",
    "table_name = \"parking_violations\"\n",
    "ny_df.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: When are tickets most likely to be issued?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on Date\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "| issued_on|tickets|\n",
      "+----------+-------+\n",
      "|08/04/2022|  66726|\n",
      "|08/05/2022|  65393|\n",
      "|08/02/2022|  64876|\n",
      "|06/30/2022|  64846|\n",
      "|07/19/2022|  64815|\n",
      "+----------+-------+\n",
      "\n",
      "\n",
      "Based on Year\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|year|tickets|\n",
      "+----+-------+\n",
      "|2022|9154317|\n",
      "|2023|2380085|\n",
      "|2021|    477|\n",
      "|2024|    117|\n",
      "|2020|     90|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Based on Date\\n\")\n",
    "q1_query = f'''\n",
    "SELECT\n",
    "  SUBSTR(issue_date, 1, 10) AS issued_on,\n",
    "  COUNT(*) AS tickets\n",
    "FROM {table_name}\n",
    "GROUP BY issued_on\n",
    "ORDER BY tickets DESC\n",
    "LIMIT 5\n",
    "'''.strip()\n",
    "\n",
    "spark.sql(q1_query).show()\n",
    "\n",
    "print(\"\\nBased on Year\\n\")\n",
    "q1_query = f'''\n",
    "SELECT\n",
    "  YEAR(TO_DATE(CAST(UNIX_TIMESTAMP(issue_date,'MM/dd/yyyy') AS timestamp))) AS year,\n",
    "  COUNT(*) AS tickets\n",
    "FROM {table_name}\n",
    "GROUP BY year\n",
    "ORDER BY tickets DESC\n",
    "LIMIT 5\n",
    "'''.strip()\n",
    "\n",
    "spark.sql(q1_query).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: What are the most common years and types of cars to be ticketed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================================>      (15 + 2) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+\n",
      "|year| make|num_tickets|\n",
      "+----+-----+-----------+\n",
      "|2021|TOYOT|     117999|\n",
      "|2019|HONDA|     113890|\n",
      "|2021|HONDA|     107202|\n",
      "|2020|HONDA|     104349|\n",
      "|2022|TOYOT|      95782|\n",
      "+----+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q2_query = f'''\n",
    "SELECT\n",
    "  vehicle_year AS year,\n",
    "  vehicle_make AS make,\n",
    "  COUNT(*) AS num_tickets\n",
    "FROM {table_name}\n",
    "WHERE vehicle_year IS NOT NULL AND vehicle_year <> 0 AND vehicle_make IS NOT NULL\n",
    "GROUP BY vehicle_year, vehicle_make\n",
    "ORDER BY num_tickets DESC\n",
    "LIMIT 5\n",
    "'''.strip()\n",
    "\n",
    "spark.sql(q2_query).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Where are tickets most commonly issued?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By violation_county\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|violation_county|tickets|\n",
      "+----------------+-------+\n",
      "|              NY|2450153|\n",
      "|              QN|1858441|\n",
      "|              BK|1732079|\n",
      "|              BX|1497854|\n",
      "|               K|1365103|\n",
      "+----------------+-------+\n",
      "\n",
      "By violation_precinct\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|violation_precinct|tickets|\n",
      "+------------------+-------+\n",
      "|                 0|5349526|\n",
      "|                19| 282466|\n",
      "|                13| 254057|\n",
      "|                 6| 224686|\n",
      "|               114| 221523|\n",
      "+------------------+-------+\n",
      "\n",
      "By violation_location\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|violation_location|tickets|\n",
      "+------------------+-------+\n",
      "|                19| 282466|\n",
      "|                13| 254057|\n",
      "|                 6| 224686|\n",
      "|               114| 221523|\n",
      "|                14| 190012|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q3_query = '''\n",
    "SELECT\n",
    "  {} AS {}, COUNT(*) AS tickets\n",
    "FROM parking_violations\n",
    "WHERE {} IS NOT NULL\n",
    "GROUP BY {}\n",
    "ORDER BY tickets DESC\n",
    "LIMIT 5\n",
    "'''.strip()\n",
    "\n",
    "print(\"By violation_county\\n\")\n",
    "spark.sql(q3_query.format(*([\"violation_county\"] * 4))).show()\n",
    "\n",
    "print(\"By violation_precinct\\n\")\n",
    "spark.sql(q3_query.format(*([\"violation_precinct\"] * 4))).show()\n",
    "\n",
    "print(\"By violation_location\\n\")\n",
    "spark.sql(q3_query.format(*([\"violation_location\"] * 4))).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which color of the vehicle is most likely to get a ticket?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============================>                          (9 + 8) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|color|num_tickets|\n",
      "+-----+-----------+\n",
      "|   GY|    2275457|\n",
      "|   WH|    2055818|\n",
      "|   BK|    1992788|\n",
      "|   BL|     760235|\n",
      "|WHITE|     671757|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q4_query = f'''\n",
    "SELECT vehicle_color AS color,\n",
    "  COUNT(*) AS num_tickets\n",
    "FROM parking_violations\n",
    "WHERE vehicle_color IS NOT NULL\n",
    "GROUP BY vehicle_color\n",
    "ORDER BY num_tickets DESC\n",
    "LIMIT 5\n",
    "'''.strip()\n",
    "spark.sql(q4_query).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Filter out null rows\n",
    "ny_df = ny_df.filter((ny_df.vehicle_color.isNotNull()) & (ny_df.street_code1.isNotNull()))\n",
    "\n",
    "# Convert `vehicle_color` to numerical feature using One-Hot Encoding\n",
    "indexer = StringIndexer(inputCol=\"vehicle_color\", outputCol=\"vehicle_color_index\")\n",
    "encoder = OneHotEncoder(inputCol=\"vehicle_color_index\", outputCol=\"vehicle_color_vec\")\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "ny_df_kmeans = pipeline.fit(ny_df).transform(ny_df).select('vehicle_color_vec', 'street_code1')\n",
    "ny_df_kmeans = VectorAssembler(inputCols=['vehicle_color_vec', 'street_code1'], outputCol=\"features\").transform(ny_df_kmeans)\n",
    "\n",
    "# Train K-Means model\n",
    "kmeans = KMeans(featuresCol='features', k=3)\n",
    "model = kmeans.fit(ny_df_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m cloudpickle\u001b[39m.\u001b[39;49mdumps(obj, pickle_protocol)\n\u001b[1;32m    459\u001b[0m \u001b[39mexcept\u001b[39;00m pickle\u001b[39m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m cp\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    603\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, types\u001b[39m.\u001b[39mFunctionType):\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_reduce(obj)\n\u001b[1;32m    693\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[39m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[1;32m    695\u001b[0m     \u001b[39m# dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dynamic_function_reduce(obj)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    545\u001b[0m newargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_getnewargs(func)\n\u001b[0;32m--> 546\u001b[0m state \u001b[39m=\u001b[39m _function_getstate(func)\n\u001b[1;32m    547\u001b[0m \u001b[39mreturn\u001b[39;00m (types\u001b[39m.\u001b[39mFunctionType, newargs, state, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m         _function_setstate)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m slotstate \u001b[39m=\u001b[39m {\n\u001b[1;32m    147\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m__closure__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__closure__\u001b[39m,\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 157\u001b[0m f_globals_ref \u001b[39m=\u001b[39m _extract_code_globals(func\u001b[39m.\u001b[39;49m\u001b[39m__code__\u001b[39;49m)\n\u001b[1;32m    158\u001b[0m f_globals \u001b[39m=\u001b[39m {k: func\u001b[39m.\u001b[39m\u001b[39m__globals__\u001b[39m[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m f_globals_ref \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m\n\u001b[1;32m    159\u001b[0m              func\u001b[39m.\u001b[39m\u001b[39m__globals__\u001b[39m}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[39m=\u001b[39m {names[oparg]: \u001b[39mNone\u001b[39;49;00m \u001b[39mfor\u001b[39;49;00m _, oparg \u001b[39min\u001b[39;49;00m _walk_global_ops(co)}\n\u001b[1;32m    336\u001b[0m \u001b[39m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m# add the result to code_globals\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[39m=\u001b[39m {names[oparg]: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m _, oparg \u001b[39min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    336\u001b[0m \u001b[39m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m Vectors\n\u001b[1;32m      3\u001b[0m \u001b[39m# Preprocess the input DataFrame\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m input_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame([(Vectors\u001b[39m.\u001b[39;49mdense([\u001b[39m1.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m]), \u001b[39m34510\u001b[39;49m), (Vectors\u001b[39m.\u001b[39;49mdense([\u001b[39m1.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m]), \u001b[39m10030\u001b[39;49m), (Vectors\u001b[39m.\u001b[39;49mdense([\u001b[39m1.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m]), \u001b[39m34050\u001b[39;49m)], [\u001b[39m\"\u001b[39;49m\u001b[39mvehicle_color_vec\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstreet_code1\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      6\u001b[0m \u001b[39m# Preprocess the input DataFrame\u001b[39;00m\n\u001b[1;32m      7\u001b[0m indexer \u001b[39m=\u001b[39m StringIndexer(inputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvehicle_color_vec\u001b[39m\u001b[39m\"\u001b[39m, outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvehicle_color_index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/session.py:938\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromLocal(\u001b[39mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39;49m_to_java_object_rdd())\n\u001b[1;32m    939\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[39m.\u001b[39mrdd(), struct\u001b[39m.\u001b[39mjson())\n\u001b[1;32m    940\u001b[0m df \u001b[39m=\u001b[39m DataFrame(jdf, \u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/rdd.py:3113\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3110\u001b[0m rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pickled()\n\u001b[1;32m   3111\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mpythonToJava(rdd\u001b[39m.\u001b[39;49m_jrdd, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3503\u001b[0m     profiler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3505\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(\n\u001b[1;32m   3506\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd_deserializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd_deserializer, profiler\n\u001b[1;32m   3507\u001b[0m )\n\u001b[1;32m   3509\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m python_rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD(\n\u001b[1;32m   3511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_jrdd\u001b[39m.\u001b[39mrdd(), wrapped_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreservesPartitioning, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_barrier\n\u001b[1;32m   3512\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[39massert\u001b[39;00m serializer, \u001b[39m\"\u001b[39m\u001b[39mserializer should not be empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3361\u001b[0m command \u001b[39m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m   3363\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3364\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonFunction(\n\u001b[1;32m   3365\u001b[0m     \u001b[39mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   3366\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3371\u001b[0m     sc\u001b[39m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   3372\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_for_python_RDD\u001b[39m(sc: \u001b[39m\"\u001b[39m\u001b[39mSparkContext\u001b[39m\u001b[39m\"\u001b[39m, command: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   3343\u001b[0m     \u001b[39m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   3344\u001b[0m     ser \u001b[39m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 3345\u001b[0m     pickled_command \u001b[39m=\u001b[39m ser\u001b[39m.\u001b[39;49mdumps(command)\n\u001b[1;32m   3346\u001b[0m     \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pickled_command) \u001b[39m>\u001b[39m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mgetBroadcastThreshold(sc\u001b[39m.\u001b[39m_jsc):  \u001b[39m# Default 1M\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m         \u001b[39m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pyspark/serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    466\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCould not serialize object: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (e\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, emsg)\n\u001b[1;32m    467\u001b[0m print_exec(sys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m--> 468\u001b[0m \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Preprocess the input DataFrame\n",
    "input_df = spark.createDataFrame([(Vectors.dense([1.0, 0.0, 0.0]), 34510), (Vectors.dense([1.0, 0.0, 0.0]), 10030), (Vectors.dense([1.0, 0.0, 0.0]), 34050)], [\"vehicle_color_vec\", \"street_code1\"])\n",
    "\n",
    "# Preprocess the input DataFrame\n",
    "indexer = StringIndexer(inputCol=\"vehicle_color_vec\", outputCol=\"vehicle_color_index\")\n",
    "indexer = StringIndexer(inputCol=\"vehicle_color_vec\", outputCol=\"vehicle_color_index\")\n",
    "encoder = OneHotEncoder(inputCol=\"vehicle_color_index\", outputCol=\"vehicle_color_vec_encoded\")\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "input_df = pipeline.fit(input_df).transform(input_df)\n",
    "\n",
    "# Assemble feature vector and predict using K-Means model\n",
    "input_data = VectorAssembler(inputCols=['vehicle_color_vec_encoded', 'street_code1'], outputCol=\"features\").transform(input_df)\n",
    "predictions = model.transform(input_data)\n",
    "\n",
    "# Calculate the probability of getting a ticket based on the cluster assignments of the input data\n",
    "cluster_assignments = predictions.select('prediction').rdd.flatMap(lambda x: x).collect()\n",
    "probabilities = model.summary.clusterSizes / sum(model.summary.clusterSizes)\n",
    "ticket_probability = probabilities[cluster_assignments[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GAME_ID: integer (nullable = true)\n",
      " |-- MATCHUP: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- W: string (nullable = true)\n",
      " |-- FINAL_MARGIN: integer (nullable = true)\n",
      " |-- SHOT_NUMBER: integer (nullable = true)\n",
      " |-- PERIOD: integer (nullable = true)\n",
      " |-- GAME_CLOCK: timestamp (nullable = true)\n",
      " |-- SHOT_CLOCK: double (nullable = true)\n",
      " |-- DRIBBLES: integer (nullable = true)\n",
      " |-- TOUCH_TIME: double (nullable = true)\n",
      " |-- SHOT_DIST: double (nullable = true)\n",
      " |-- PTS_TYPE: integer (nullable = true)\n",
      " |-- SHOT_RESULT: string (nullable = true)\n",
      " |-- CLOSEST_DEFENDER: string (nullable = true)\n",
      " |-- CLOSEST_DEFENDER_PLAYER_ID: integer (nullable = true)\n",
      " |-- CLOSE_DEF_DIST: double (nullable = true)\n",
      " |-- FGM: integer (nullable = true)\n",
      " |-- PTS: integer (nullable = true)\n",
      " |-- player_name: string (nullable = true)\n",
      " |-- player_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nba = spark.read.options(header=True, inferschema=True).csv(\"shot_logs.csv\")\n",
    "nba.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _For each pair of the players (A, B), we define the fear sore of A when facing B is the hit_\n",
    "## _rate, such that B is closet defender when A is shooting. Based on the fear sore, for each_\n",
    "## _player, please find out who is his ”most unwanted defender”_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temp. VIEW for Spark SQl\n",
    "table_name = \"shots\"\n",
    "nba.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we will group the data by the player and the closest defender and aggregate the shots that they have missed and hit for that defender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+----+------+\n",
      "|     player_name|  closest_defender|hits|misses|\n",
      "+----------------+------------------+----+------+\n",
      "|   brian roberts|        Gasol, Pau|   0|     1|\n",
      "|    al jefferson| Hardaway Jr., Tim|   0|     1|\n",
      "|     cody zeller|     Price, Ronnie|   0|     1|\n",
      "|       gary neal|     Beal, Bradley|   3|     3|\n",
      "|       gary neal|     Smart, Marcus|   0|     4|\n",
      "|gerald henderson|    Bazemore, Kent|   0|     2|\n",
      "|    kemba walker|     Williams, Lou|   1|     2|\n",
      "|lance stephenson|    Fournier, Evan|   0|     2|\n",
      "| marvin williams| Early, Cleanthony|   1|     1|\n",
      "|  gordon hayward|Aldridge, LaMarcus|   2|     5|\n",
      "|  gordon hayward|    Bazemore, Kent|   1|     1|\n",
      "|   trevor booker|   Thompson, Jason|   3|     3|\n",
      "|   trevor booker|    Frye, Channing|   1|     2|\n",
      "|     enes kanter|   Chandler, Tyson|   1|     6|\n",
      "|      dante exum|      Williams, Mo|   0|     3|\n",
      "|      jon ingles|     Jack, Jarrett|   1|     1|\n",
      "|      jon ingles|     Williams, Lou|   1|     5|\n",
      "|     rudy gobert|       Foye, Randy|   1|     1|\n",
      "|     rudy gobert|      Wade, Dwyane|   1|     1|\n",
      "|   carlos boozer|     Jefferson, Al|   2|     2|\n",
      "+----------------+------------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate hits and misses based on SHOT_RESULT\n",
    "agg = nba.groupBy(\"player_name\",\"closest_defender\")\\\n",
    "    .agg(count(when(nba[\"SHOT_RESULT\"]==\"made\", True))\\\n",
    "    .alias('hits'),count(\"SHOT_RESULT\").alias('misses'))\n",
    "agg.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To calculate the hit rate, we can use the following equation\n",
    "### hit_rate = hits / (hits + misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----+------+--------+\n",
      "| player_name| closest_defender|hits|misses|hit_rate|\n",
      "+------------+-----------------+----+------+--------+\n",
      "|aaron brooks|   Thompson, Klay|   1|     3|    0.25|\n",
      "|aaron brooks|Livingston, Shaun|   1|     2|    0.33|\n",
      "|aaron brooks|     Smith, Jason|   1|     2|    0.33|\n",
      "|aaron brooks|    Lee, Courtney|   1|     3|    0.25|\n",
      "|aaron brooks| Carroll, DeMarre|   1|     1|     0.5|\n",
      "|aaron brooks|    Nurkic, Jusuf|   0|     2|     0.0|\n",
      "|aaron brooks|     Lopez, Robin|   2|     3|     0.4|\n",
      "|aaron brooks|    Harris, Devin|   0|     1|     0.0|\n",
      "|aaron brooks|      Green, Jeff|   0|     1|     0.0|\n",
      "|aaron brooks|     LaVine, Zach|   1|     3|    0.25|\n",
      "|aaron brooks|    Ariza, Trevor|   1|     1|     0.5|\n",
      "|aaron brooks|   Hayes, Charles|   0|     2|     0.0|\n",
      "|aaron brooks| Williams, Marvin|   1|     2|    0.33|\n",
      "|aaron brooks|  Napier, Shabazz|   4|     6|     0.4|\n",
      "|aaron brooks|  Lillard, Damian|   3|     6|    0.33|\n",
      "|aaron brooks|      Scola, Luis|   0|     1|     0.0|\n",
      "|aaron brooks|      Gasol, Marc|   1|     1|     0.5|\n",
      "|aaron brooks|      Exum, Dante|   0|     3|     0.0|\n",
      "|aaron brooks|       Lawson, Ty|   0|     5|     0.0|\n",
      "|aaron brooks|    Green, Willie|   1|     2|    0.33|\n",
      "+------------+-----------------+----+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg = agg.withColumn('hit_rate', round(agg['hits']/(agg['hits'] + agg['misses']), 2))\n",
    "agg.orderBy(agg['player_name']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove duplicate values for the hit_rate we will rank the data based on player_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----+------+--------+----+\n",
      "| player_name|    closest_defender|hits|misses|hit_rate|rank|\n",
      "+------------+--------------------+----+------+--------+----+\n",
      "|aaron brooks|       Nurkic, Jusuf|   0|     2|     0.0|   1|\n",
      "|aaron brooks|       Harris, Devin|   0|     1|     0.0|   2|\n",
      "|aaron brooks|         Green, Jeff|   0|     1|     0.0|   3|\n",
      "|aaron brooks|      Hayes, Charles|   0|     2|     0.0|   4|\n",
      "|aaron brooks|         Scola, Luis|   0|     1|     0.0|   5|\n",
      "|aaron brooks|         Exum, Dante|   0|     3|     0.0|   6|\n",
      "|aaron brooks|          Lawson, Ty|   0|     5|     0.0|   7|\n",
      "|aaron brooks|     Crawford, Jamal|   0|     1|     0.0|   8|\n",
      "|aaron brooks|      Fournier, Evan|   0|     1|     0.0|   9|\n",
      "|aaron brooks|       O'Quinn, Kyle|   0|     1|     0.0|  10|\n",
      "|aaron brooks|        Wear, Travis|   0|     1|     0.0|  11|\n",
      "|aaron brooks|   Dos Santos, Atila|   0|     1|     0.0|  12|\n",
      "|aaron brooks|        Hairston, PJ|   0|     1|     0.0|  13|\n",
      "|aaron brooks|     Johnson, Wesley|   0|     1|     0.0|  14|\n",
      "|aaron brooks|      Powell, Dwight|   0|     1|     0.0|  15|\n",
      "|aaron brooks|      Rivers, Austin|   0|     2|     0.0|  16|\n",
      "|aaron brooks|Antetokounmpo, Gi...|   0|     1|     0.0|  17|\n",
      "|aaron brooks|     Splitter, Tiago|   0|     1|     0.0|  18|\n",
      "|aaron brooks|      Johnson, Chris|   0|     2|     0.0|  19|\n",
      "|aaron brooks|    Schroder, Dennis|   0|     1|     0.0|  20|\n",
      "+------------+--------------------+----+------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg = agg.withColumn(\"rank\",row_number().over(Window.partitionBy(\"player_name\")\\\n",
    "    .orderBy(agg[\"hit_rate\"].asc())))\n",
    "agg.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we will only get those ranks which are 1 to get the 'Most Unwanted Defender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----+\n",
      "|     player_name|most_unwanted_defender|rank|\n",
      "+----------------+----------------------+----+\n",
      "|    aaron brooks|         Nurkic, Jusuf|   1|\n",
      "|    aaron gordon|        Rivers, Austin|   1|\n",
      "| al farouq aminu|        Johnson, James|   1|\n",
      "|      al horford|           Diaw, Boris|   1|\n",
      "|    al jefferson|     Hardaway Jr., Tim|   1|\n",
      "|   alan anderson|            Leuer, Jon|   1|\n",
      "|     alan crabbe|      Sefolosha, Thabo|   1|\n",
      "|        alex len|       Knight, Brandon|   1|\n",
      "|   alexis ajinca|          Meeks, Jodie|   1|\n",
      "|      alonzo gee|          Korver, Kyle|   1|\n",
      "|amare stoudemire|            Deng, Luol|   1|\n",
      "|    amir johnson|         Grant, Jerami|   1|\n",
      "|  andre drummond|         James, LeBron|   1|\n",
      "|  andre iguodala|           Lowry, Kyle|   1|\n",
      "|    andre miller|          Turner, Evan|   1|\n",
      "|  andre roberson|        Ginobili, Manu|   1|\n",
      "|    andrew bogut|          Ibaka, Serge|   1|\n",
      "|  andrew wiggins|        Sanders, Larry|   1|\n",
      "| anthony bennett|        Ajinca, Alexis|   1|\n",
      "|   anthony davis|     Mbah a Moute, Luc|   1|\n",
      "+----------------+----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mud = agg.where(agg['rank'] == 1).withColumnRenamed(\"closest_defender\", \"most_unwanted_defender\")\\\n",
    "    .select(\"player_name\", \"most_unwanted_defender\", \"rank\")\n",
    "mud.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _For each player, we define the comfortable zone of shooting is a matrix of,_\n",
    "## _{SHOT DIST, CLOSE DEF DIST, SHOT CLOCK}_\n",
    "## _Please develop a Spark-based algorithm to classify each player’s records into 4 comfort-_\n",
    "## _able zones. Considering the hit rate, which zone is the best for James Harden, Chris Paul, Stephen Curry, and Lebron James._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To calculate comfortable zones for these players, we can make use of K-Means clustering where each cluster maps to a zone. Since we need 4 zones, there will be 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+--------------+----------+\n",
      "|         player_name|shot_result|shot_dist|close_def_dist|shot_clock|\n",
      "+--------------------+-----------+---------+--------------+----------+\n",
      "|       brian roberts|     missed|     19.6|           4.6|      16.7|\n",
      "|       brian roberts|     missed|     23.8|           5.5|      17.6|\n",
      "|       brian roberts|       made|     20.5|           5.3|      14.4|\n",
      "|       brian roberts|     missed|     24.5|           5.2|      15.3|\n",
      "|        al jefferson|       made|     19.3|           7.6|      10.0|\n",
      "|        al jefferson|       made|      7.7|           2.2|       3.8|\n",
      "|         cody zeller|     missed|     18.1|           4.9|      12.1|\n",
      "|         cody zeller|     missed|      3.1|           2.4|      24.0|\n",
      "|           gary neal|       made|     17.4|           3.1|      11.0|\n",
      "|           gary neal|     missed|     25.5|           5.5|      11.8|\n",
      "|           gary neal|     missed|      6.5|           0.2|      14.8|\n",
      "|    gerald henderson|       made|      5.3|           0.8|      20.8|\n",
      "|    gerald henderson|     missed|     22.5|           6.9|      12.7|\n",
      "|    gerald henderson|       made|      4.9|           1.7|      15.0|\n",
      "|    gerald henderson|     missed|     20.6|           5.1|      13.3|\n",
      "|        kemba walker|     missed|     15.3|           3.4|      18.0|\n",
      "|        kemba walker|     missed|     24.1|           2.8|       1.0|\n",
      "|        kemba walker|       made|     16.2|           3.9|      13.1|\n",
      "|michael kidd-gilc...|       made|      4.0|           2.9|      13.6|\n",
      "|    lance stephenson|       made|      3.2|           3.9|      17.9|\n",
      "+--------------------+-----------+---------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones = nba.select('player_name','shot_result','shot_dist','close_def_dist','shot_clock').dropna().dropDuplicates()\n",
    "zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['shot_dist','close_def_dist','shot_clock'], outputCol=\"features\",handleInvalid=\"skip\")\n",
    "kmeans = KMeans(featuresCol='features', k=4, seed=10)    \n",
    "t_data = assembler.transform(zones)\n",
    "op_fit = kmeans.fit(t_data)\n",
    "zones = op_fit.transform(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+--------------+----------+---------------+----------+\n",
      "|         player_name|shot_result|shot_dist|close_def_dist|shot_clock|       features|prediction|\n",
      "+--------------------+-----------+---------+--------------+----------+---------------+----------+\n",
      "|       brian roberts|     missed|     19.6|           4.6|      16.7|[19.6,4.6,16.7]|         1|\n",
      "|       brian roberts|     missed|     23.8|           5.5|      17.6|[23.8,5.5,17.6]|         1|\n",
      "|       brian roberts|       made|     20.5|           5.3|      14.4|[20.5,5.3,14.4]|         1|\n",
      "|       brian roberts|     missed|     24.5|           5.2|      15.3|[24.5,5.2,15.3]|         1|\n",
      "|        al jefferson|       made|     19.3|           7.6|      10.0|[19.3,7.6,10.0]|         2|\n",
      "|        al jefferson|       made|      7.7|           2.2|       3.8|  [7.7,2.2,3.8]|         0|\n",
      "|         cody zeller|     missed|     18.1|           4.9|      12.1|[18.1,4.9,12.1]|         1|\n",
      "|         cody zeller|     missed|      3.1|           2.4|      24.0| [3.1,2.4,24.0]|         3|\n",
      "|           gary neal|       made|     17.4|           3.1|      11.0|[17.4,3.1,11.0]|         2|\n",
      "|           gary neal|     missed|     25.5|           5.5|      11.8|[25.5,5.5,11.8]|         1|\n",
      "|           gary neal|     missed|      6.5|           0.2|      14.8| [6.5,0.2,14.8]|         3|\n",
      "|    gerald henderson|       made|      5.3|           0.8|      20.8| [5.3,0.8,20.8]|         3|\n",
      "|    gerald henderson|     missed|     22.5|           6.9|      12.7|[22.5,6.9,12.7]|         1|\n",
      "|    gerald henderson|       made|      4.9|           1.7|      15.0| [4.9,1.7,15.0]|         3|\n",
      "|    gerald henderson|     missed|     20.6|           5.1|      13.3|[20.6,5.1,13.3]|         1|\n",
      "|        kemba walker|     missed|     15.3|           3.4|      18.0|[15.3,3.4,18.0]|         1|\n",
      "|        kemba walker|     missed|     24.1|           2.8|       1.0| [24.1,2.8,1.0]|         2|\n",
      "|        kemba walker|       made|     16.2|           3.9|      13.1|[16.2,3.9,13.1]|         1|\n",
      "|michael kidd-gilc...|       made|      4.0|           2.9|      13.6| [4.0,2.9,13.6]|         0|\n",
      "|    lance stephenson|       made|      3.2|           3.9|      17.9| [3.2,3.9,17.9]|         3|\n",
      "+--------------------+-----------+---------+--------------+----------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we can calculate the hit_rate like we did for the previous question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 401:=========================================================(3 + 0) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+--------+\n",
      "|prediction| hits|misses|hit_rate|\n",
      "+----------+-----+------+--------+\n",
      "|         1|12692| 32309|    0.28|\n",
      "|         3|15779| 26674|    0.37|\n",
      "|         2|10589| 28652|    0.27|\n",
      "|         0|16801| 34821|    0.33|\n",
      "+----------+-----+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "zones = zones.groupBy(\"prediction\")\\\n",
    "    .agg(count(when(zones[\"SHOT_RESULT\"]==\"made\", True))\\\n",
    "    .alias('hits'),count(\"SHOT_RESULT\").alias('misses'))\n",
    "zones = zones.withColumn('hit_rate', round(zones['hits']/(zones['hits'] + zones['misses']), 2))\n",
    "zones.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the previous dataframe, let us fetch the hit rates for James Harden, Chris Paul, Stephen Curry, and Lebron James"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----+------+--------+----+\n",
      "|player_name|    closest_defender|hits|misses|hit_rate|rank|\n",
      "+-----------+--------------------+----+------+--------+----+\n",
      "| chris paul|       Brewer, Corey|   0|     1|     0.0|   1|\n",
      "| chris paul|    Prince, Tayshaun|   0|     1|     0.0|   2|\n",
      "| chris paul|        Ennis, James|   0|     1|     0.0|   3|\n",
      "| chris paul|       Marion, Shawn|   0|     1|     0.0|   4|\n",
      "| chris paul|      Shumpert, Iman|   0|     1|     0.0|   5|\n",
      "| chris paul|   Livingston, Shaun|   0|     1|     0.0|   6|\n",
      "| chris paul|     Arthur, Darrell|   0|     1|     0.0|   7|\n",
      "| chris paul|     Leonard, Meyers|   0|     2|     0.0|   8|\n",
      "| chris paul|     Williams, Deron|   0|     1|     0.0|   9|\n",
      "| chris paul|Dellavedova, Matthew|   0|     1|     0.0|  10|\n",
      "| chris paul|        Dorsey, Joey|   0|     1|     0.0|  11|\n",
      "| chris paul|       Waiters, Dion|   0|     1|     0.0|  12|\n",
      "| chris paul|    Carroll, DeMarre|   0|     2|     0.0|  13|\n",
      "| chris paul|   Williams, Derrick|   0|     1|     0.0|  14|\n",
      "| chris paul|    Bargnani, Andrea|   0|     3|     0.0|  15|\n",
      "| chris paul|    Barea, Jose Juan|   0|     1|     0.0|  16|\n",
      "| chris paul|          Mayo, O.J.|   0|     1|     0.0|  17|\n",
      "| chris paul|   Beverley, Patrick|   0|     6|     0.0|  18|\n",
      "| chris paul|  Valanciunas, Jonas|   0|     4|     0.0|  19|\n",
      "| chris paul|      Thomas, Isaiah|   0|     1|     0.0|  20|\n",
      "+-----------+--------------------+----+------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg = agg.where(agg['player_name'].isin(['james harden','chris paul','stephen curry','lebron james']))\n",
    "agg.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
